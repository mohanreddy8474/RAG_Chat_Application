{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxyLvwx25U-A"
      },
      "outputs": [],
      "source": [
        "!pip install pinecone-client\n",
        "!pip install protoc_gen_openapiv2\n",
        "!pip install -q -U google-generativeai\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8tbGHjvpVPk"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "from pinecone.grpc import PineconeGRPC as Pinecone\n",
        "from pinecone import ServerlessSpec\n",
        "from typing import List, Dict\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jIqeTAbx5Y8p"
      },
      "outputs": [],
      "source": [
        "genai.configure(api_key='Your API KEY')\n",
        "pc = Pinecone(api_key='Your API KEY')\n",
        "model = genai.GenerativeModel('gemini-1.0-pro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t0Yr8vwY-M9L"
      },
      "outputs": [],
      "source": [
        "MAX_TOKENS = 225\n",
        "PINECONE_INDEX_NAME = 'chat-index'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aVKIZtxq-aNy"
      },
      "outputs": [],
      "source": [
        "history = [\n",
        "    \"1: User: Hi there! How are you doing today? | Bot: Hello! I'm doing great, thank you! How can I assist you today?\",\n",
        "    \"2: User: What's the best way to start learning Python? | Bot: Start with online resources like Codecademy or Coursera. They offer great introductory courses.\",\n",
        "    \"3: User: Sounds good! What about practice projects? | Bot: You can build a simple calculator or a to-do list app to practice your skills.\",\n",
        "    \"4: User: I'll give that a try. Any good resources for learning web development? | Bot: FreeCodeCamp and Mozilla's MDN Web Docs are excellent places to start.\",\n",
        "    \"5: User: Great! What should I focus on first? | Bot: Begin with HTML and CSS, then move on to JavaScript for interactive elements.\",\n",
        "    \"6: User: Thanks! What about backend development? | Bot: Learn about server-side languages like Python with Flask or Django, or Node.js with Express.\",\n",
        "    \"7: User: I'll look into those. Any tips for debugging code? | Bot: Use tools like the Chrome DevTools for front-end and pdb for Python debugging.\",\n",
        "    \"8: User: Those sound helpful. What about version control? | Bot: Git is essential. Platforms like GitHub or GitLab are great for managing your code.\",\n",
        "    \"9: User: I'll start using Git. Any good tech blogs or websites? | Bot: Sites like TechCrunch, Hacker News, and Medium have valuable tech content.\",\n",
        "    \"10: User: I'll check them out. What about staying updated with tech news? | Bot: Subscribe to newsletters like The Verge, Wired, or Ars Technica.\",\n",
        "    \"11: User: Good idea. What are some popular tech podcasts? | Bot: 'Reply All,' 'The Vergecast,' and 'Coding Blocks' are popular tech podcasts.\",\n",
        "    \"12: User: I'll listen to those. Any recommendations for open-source projects to contribute to? | Bot: Look for projects on GitHub tagged with 'good first issue' to start contributing.\",\n",
        "    \"13: User: I'll find some to contribute to. What are some must-know algorithms? | Bot: Understand sorting algorithms like QuickSort and MergeSort, and data structures like linked lists and hash tables.\",\n",
        "    \"14: User: I'll study those. Any tips for coding interviews? | Bot: Practice problems on LeetCode or HackerRank, and understand the basics of data structures and algorithms.\",\n",
        "    \"15: User: I'll prepare with those resources. What about learning DevOps? | Bot: Learn about Docker, Kubernetes, and CI/CD tools like Jenkins.\",\n",
        "    \"16: User: Those seem important. Any good books on software engineering? | Bot: 'Clean Code' by Robert C. Martin and 'The Pragmatic Programmer' by Andrew Hunt and David Thomas are highly recommended.\",\n",
        "    \"17: User: I'll read those. What about good practices for remote work? | Bot: Maintain a regular schedule, use tools like Slack for communication, and have a dedicated workspace.\",\n",
        "    \"18: User: Those are great tips. What about staying productive? | Bot: Use techniques like Pomodoro, and tools like Trello or Asana to manage tasks.\",\n",
        "    \"19: User: I'll try those out. Any last advice for a tech career? | Bot: Keep learning, stay curious, and network with other professionals in the field.\",\n",
        "    \"20: User: Thanks for all the advice! I'll make the most out of it. | Bot: You're welcome! Enjoy your journey and best of luck with your tech career!\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qDbldLfGxXfu"
      },
      "outputs": [],
      "source": [
        "# Function to generate embeddings\n",
        "def generate_embedding(text: str):\n",
        "        result = genai.embed_content(\n",
        "          model=\"models/text-embedding-004\",\n",
        "          content=text,\n",
        "          task_type=\"retrieval_document\",\n",
        "          title=\"Embedding of single string\")\n",
        "        return result['embedding']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izrwIeBk-ilb"
      },
      "outputs": [],
      "source": [
        "def add_embeddings_to_pinecone(history, index_name='chat-history'):\n",
        "    \"\"\"\n",
        "    This function should:\n",
        "    1. Encode each message in the history using a transformer model.\n",
        "    2. Include the message number as part of the embedding.\n",
        "    3. Create a Pinecone index if it does not exist.\n",
        "    4. Upsert the encoded messages into the Pinecone index.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Create Pinecone index if it does not exist\n",
        "    index_list = pc.list_indexes().get('indexes', [])\n",
        "    chat_history_exists = any(index['name'] == 'chat-history' for index in index_list)\n",
        "    if not chat_history_exists:\n",
        "        pc.create_index(index_name,\n",
        "            dimension=768,\n",
        "            metric=\"cosine\",\n",
        "            spec=ServerlessSpec(\n",
        "            cloud='aws',\n",
        "            region='us-east-1'\n",
        "        ))  # Assuming embeddings are 768-dimensional\n",
        "    index = pc.Index(index_name)\n",
        "\n",
        "\n",
        "\n",
        "    # Parse history and encode messages\n",
        "    vectors = []\n",
        "    for item in history:\n",
        "        # Split user and bot messages\n",
        "        message_number, conversation = item.split(\": \", 1)\n",
        "        user_message, bot_message = conversation.split(\" | Bot: \")\n",
        "        user_message = user_message.replace(\"User: \", \"\")\n",
        "\n",
        "        # Encode messages\n",
        "        user_embedding = generate_embedding(user_message)\n",
        "        bot_embedding = generate_embedding(bot_message)\n",
        "\n",
        "        # Create metadata\n",
        "        user_metadata = {\"role\": \"user\", \"content\": user_message, \"message_number\": message_number}\n",
        "        bot_metadata = {\"role\": \"assistant\", \"content\": bot_message, \"message_number\": message_number}\n",
        "\n",
        "        # Append to vectors\n",
        "        vectors.append({\"id\": f\"user-{message_number}\", \"values\": user_embedding, \"metadata\": user_metadata})\n",
        "        vectors.append({\"id\": f\"bot-{message_number}\", \"values\": bot_embedding, \"metadata\": bot_metadata})\n",
        "\n",
        "    # Upsert vectors into Pinecone\n",
        "    index.upsert(vectors, namespace=\"history\")\n",
        "\n",
        "\n",
        "add_embeddings_to_pinecone(history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MeIwZgxasWoM"
      },
      "outputs": [],
      "source": [
        "def retrieve_relevant_history(query,index_name='chat-history'):\n",
        "    \"\"\"\n",
        "    This function should:\n",
        "    1. Encode the query using a transformer model.\n",
        "    2. Query the Pinecone index with the encoded query to retrieve the most relevant messages.\n",
        "    3. Return the relevant messages.\n",
        "    \"\"\"\n",
        "\n",
        "    #Initialize pinecone Index\n",
        "    index = pc.Index('chat-history')\n",
        "\n",
        "    #Generate embeddings for query\n",
        "    query_embedding = generate_embedding(query)\n",
        "\n",
        "    # Query Pinecone index for relevant messages\n",
        "    query_response = index.query(namespace='history',vector=query_embedding, top_k=5, include_metadata=True)\n",
        "\n",
        "\n",
        "    # Extract and return relevant messages\n",
        "    relevant_messages = []\n",
        "    for match in query_response['matches']:\n",
        "        relevant_messages.append(match['metadata']['content'])\n",
        "    return relevant_messages\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rAFMLAjN99XP"
      },
      "outputs": [],
      "source": [
        "def prepare_prompt(test_prompt, index_name='chat-history'):\n",
        "    \"\"\"\n",
        "    This function should:\n",
        "    1. Retrieve relevant history messages using the RAG mechanism.\n",
        "    2. Combine the retrieved messages with the test prompt.\n",
        "    3. Ensure the combined prompt does not exceed the maximum token limit.\n",
        "    4. Return the combined prompt and the context referred.\n",
        "    \"\"\"\n",
        "    # Retrieve relevant history\n",
        "    relevant_history = retrieve_relevant_history(test_prompt, index_name=index_name)\n",
        "\n",
        "    # Combine the retrieved messages with the test prompt\n",
        "    combined_prompt = \"\\n\".join(relevant_history) + \"\\n\" + test_prompt\n",
        "\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    tokens = encoding.encode(combined_prompt)\n",
        "\n",
        "    if len(tokens) > MAX_TOKENS:\n",
        "      # Truncate the combined prompt to fit within the token limit\n",
        "      num_tokens_to_remove = len(tokens) - MAX_TOKENS\n",
        "      truncated_tokens = tokens[:-num_tokens_to_remove]\n",
        "      combined_text = encoding.decode(truncated_tokens)\n",
        "\n",
        "\n",
        "    return combined_prompt, relevant_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHMmcG0WDm17"
      },
      "outputs": [],
      "source": [
        "def test_final_prompt():\n",
        "    \"\"\"\n",
        "    This function should:\n",
        "    1. Define the final test prompt.\n",
        "    2. Prepare the prompt using the prepare_prompt function.\n",
        "    3. Call OpenAI to generate a response.\n",
        "    4. Print the final test prompt, the chat history context referred to, and the final response.\n",
        "    5. Validate manually if the final response references the correct context.\n",
        "    \"\"\"\n",
        "    final_test_prompt = \"How to learn Python\"\n",
        "\n",
        "    # Prepare the prompt using the prepare_prompt function\n",
        "    prepared_prompt, context_referred = prepare_prompt(final_test_prompt)\n",
        "    print(prepared_prompt,\"promt\")\n",
        "\n",
        "    # Get the chat completion response from OpenAI\n",
        "    response = model.generate_content(prepared_prompt)\n",
        "\n",
        "    print(f\"Final Test Prompt: {final_test_prompt}\")\n",
        "    print(f\"Context Referred: {context_referred}\")\n",
        "    print(f\"Final Test Prompt Response: {response.text}\")\n",
        "\n",
        "# Call the test function to generate the Final Test Prompt Response\n",
        "test_final_prompt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jum6cNr5Ef31"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
